{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nature Conservancy Fish Classification - Bounding Box End-to-End Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX 980M (CNMeM is enabled with initial size: 90.0% of memory, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import GlobalAveragePooling2D, Activation, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from vgg16bn import Vgg16BN as VggConv\n",
    "\n",
    "from utils import * \n",
    "from models import Vgg16BN, Inception, Resnet50\n",
    "from glob import iglob\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_HOME_DIR = ROOT_DIR + '/data'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "data_path = DATA_HOME_DIR + '/cropped/' \n",
    "split_train_path = data_path + 'train/'\n",
    "valid_path = data_path + 'valid/'\n",
    "test_path = DATA_HOME_DIR + '/test/'\n",
    "saved_model_path = 'models/sliding_window/'\n",
    "pretrained_model_path = \"models/bb_end_to_end/0.31-loss_8epoch_224x224_aug_0.001lr_run0_vggbn.h5\"\n",
    "submission_path = 'submissions/sliding_window/'\n",
    "fish_detector_path = 'models/fish_detector_480x270/0.03-loss_2epoch_480x270_0.3-dropout_0.001-lr_vggbn.h5'\n",
    "\n",
    "# data\n",
    "batch_size = 16\n",
    "im_size = (224, 224)  # (ht, wt); only 299x299 for inception\n",
    "nb_split_train_samples = 2847\n",
    "nb_valid_samples = 450\n",
    "nb_test_samples = 1000\n",
    "classes = [\"ALB\", \"BET\", \"DOL\", \"LAG\", \"OTHER\", \"SHARK\", \"YFT\"]\n",
    "nb_classes = len(classes)\n",
    "\n",
    "# model\n",
    "nb_runs = 1\n",
    "nb_epoch = 12\n",
    "nb_aug = 1\n",
    "dropout = 0.5\n",
    "lr=0.001           \n",
    "clip = 0.01\n",
    "archs = [\"vggbn\"]\n",
    "WINDOW_SIZE = 224\n",
    "\n",
    "models = {\n",
    "    \"vggbn\": Vgg16BN(size=im_size, n_classes=nb_classes, lr=lr,\n",
    "                           batch_size=batch_size, dropout=dropout),\n",
    "    \"inception\": Inception(size=(299, 299), n_classes=nb_classes,\n",
    "                           lr=0.001, batch_size=batch_size),\n",
    "    \"resnet\": Resnet50(size=im_size, n_classes=nb_classes, lr=lr,\n",
    "                    batch_size=batch_size, dropout=dropout)\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build & Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(parent_model, model_str):\n",
    "    parent_model.build()    \n",
    "    model_fn = saved_model_path + '{val_loss:.2f}-loss_{epoch}epoch_' + model_str\n",
    "    ckpt = ModelCheckpoint(filepath=model_fn, monitor='val_loss',\n",
    "                           save_best_only=True, save_weights_only=True)\n",
    "    \n",
    "    parent_model.fit_val(split_train_path, valid_path, nb_trn_samples=nb_split_train_samples, \n",
    "                         nb_val_samples=nb_valid_samples, nb_epoch=nb_epoch, callbacks=[ckpt], aug=nb_aug)\n",
    "\n",
    "    model_path = max(iglob(saved_model_path + '*.h5'), key=os.path.getctime)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_all():    \n",
    "    model_paths = {\n",
    "        \"vggbn\": [],\n",
    "        \"inception\": [],\n",
    "        'resnet': [],\n",
    "    }\n",
    "    \n",
    "    for run in range(nb_runs):\n",
    "        print(\"Starting Training Run {0} of {1}...\\n\".format(run+1, nb_runs))\n",
    "        aug_str = \"aug\" if nb_aug else \"no-aug\"\n",
    "        \n",
    "        for arch in archs:\n",
    "            print(\"Training {} model...\\n\".format(arch))\n",
    "            model = models[arch]\n",
    "            model_str = \"{0}x{1}_{2}_{3}lr_run{4}_{5}.h5\".format(model.size[0], model.size[1], aug_str,\n",
    "                                                                 model.lr, run, arch)\n",
    "            model_path = train(model, model_str)\n",
    "            model_paths[arch].append(model_path)\n",
    "        \n",
    "    print(\"Done.\") \n",
    "    return model_paths\n",
    "        \n",
    "# model_paths = train_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_preds(model, step=40, win_size=WINDOW_SIZE):\n",
    "    fish_detector = Vgg16BN(size=(270, 480), n_classes=2, lr=0.001,\n",
    "                            batch_size=batch_size, dropout=dropout)\n",
    "    fish_detector.build()\n",
    "    fish_detector.model.load_weights(fish_detector_path)\n",
    "\n",
    "    nofish_prob, filenames = fish_detector.test(test_path, nb_test_samples, aug=nb_aug)\n",
    "    nofish_prob = nofish_prob[:, 1]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for ix, fn in enumerate(filenames):\n",
    "        \n",
    "        if ix % 100 == 0:\n",
    "            print(\"Processing {0} of {1}\".format(ix, len(filenames)))\n",
    "        \n",
    "        im_id = fn.split(\"/\")[-1]\n",
    "        im = img_to_array(load_img(test_path + fn))\n",
    "    \n",
    "        # Nofish\n",
    "        if nofish_prob[ix] > 0.5:\n",
    "            alloc = (1. - nofish_prob[ix] / 7.)\n",
    "            pred = np.array([alloc, alloc, alloc, alloc, nofish_prob[ix], alloc, alloc, alloc])\n",
    "            predictions.append(pred)\n",
    "            \n",
    "        # Fish\n",
    "        else:\n",
    "            fish_preds = np.zeros(nb_classes)\n",
    "            box_ix = 0\n",
    "            \n",
    "            for top in range(0, im.shape[1] - win_size + 1, step):\n",
    "                for left in range(0, im.shape[2] - win_size + 1, step):\n",
    "                    # compute the (top, left, bottom, right) of the bounding box\n",
    "                    box = (top, left, top + win_size, left + win_size)\n",
    "\n",
    "                    # crop the original image\n",
    "                    cropped_img = im[:, box[0]:box[2], box[1]:box[3]].reshape(1, 3, im_size[0], im_size[1])\n",
    "\n",
    "                    fish_preds += model.predict(cropped_img)[0]\n",
    "                    box_ix += 1\n",
    "                    \n",
    "            fish_preds /= box_ix\n",
    "            \n",
    "            fish_preds = np.expand_dims(fish_preds, axis=0)\n",
    "                    \n",
    "            weight = -1. * (nofish_prob[ix] - 1.)\n",
    "            fish_preds = weight * fish_preds\n",
    "            pred = np.insert(fish_preds, 4, nofish_prob[ix], axis=1)\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    return np.array(predictions), filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Predicting on vggbn model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'ds' parameter is not going to exist anymore as it is going to be replaced by the parameter 'ws'.\n",
      "  mode='max')\n",
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'st' parameter is not going to exist anymore as it is going to be replaced by the parameter 'stride'.\n",
      "  mode='max')\n",
      "/home/robert/anaconda3/lib/python3.5/site-packages/keras/backend/theano_backend.py:1500: UserWarning: DEPRECATION: the 'padding' parameter is not going to exist anymore as it is going to be replaced by the parameter 'pad'.\n",
      "  mode='max')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 1 classes.\n",
      "Processing 0 of 1000\n",
      "Processing 100 of 1000\n",
      "Processing 200 of 1000\n",
      "Processing 300 of 1000\n",
      "Processing 400 of 1000\n",
      "Processing 500 of 1000\n",
      "Processing 600 of 1000\n",
      "Processing 700 of 1000\n",
      "Processing 800 of 1000\n",
      "Processing 900 of 1000\n"
     ]
    }
   ],
   "source": [
    "def test(model_paths):   \n",
    "                 \n",
    "    print(\"----Predicting on {} model...\".format(archs[0]))\n",
    "    parent = models[\"vggbn\"]\n",
    "    model = parent.build()\n",
    "    model.load_weights(model_paths[archs[0]][0])\n",
    "    pred, filenames = generate_preds(model)\n",
    "    \n",
    "    predictions = []\n",
    "\n",
    "    # weird, hacky reshaping\n",
    "    for p in pred:\n",
    "        if p.shape != (1, 8):\n",
    "            p = p.reshape(1, 8)\n",
    "        predictions.append(p)\n",
    "        \n",
    "    predictions = np.concatenate([p for p in predictions])\n",
    "    return predictions, filenames\n",
    "\n",
    "predictions, filenames = test(model_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Predictions to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Predictions to CSV...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def write_submission(predictions, filenames):\n",
    "    preds = np.clip(predictions, clip, 1-clip)\n",
    "    sub_fn = submission_path + '{0}epoch_{1}aug_{2}clip_{3}runs'.format(nb_epoch, nb_aug, clip, nb_runs)\n",
    "    \n",
    "    for arch in archs:\n",
    "        sub_fn += \"_{}\".format(arch)\n",
    "\n",
    "    with open(sub_fn + '.csv', 'w') as f:\n",
    "        print(\"Writing Predictions to CSV...\")\n",
    "        f.write('image,ALB,BET,DOL,LAG,NoF,OTHER,SHARK,YFT\\n')\n",
    "        for i, image_name in enumerate(filenames):\n",
    "            pred = ['%.6f' % p for p in preds[i, :]]\n",
    "            f.write('%s,%s\\n' % (os.path.basename(image_name), ','.join(pred)))\n",
    "        print(\"Done.\")\n",
    "\n",
    "write_submission(predictions, filenames)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
